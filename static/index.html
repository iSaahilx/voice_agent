<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Low-Latency Voice Agent - Deepgram</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }

        .container {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            max-width: 900px;
            width: 100%;
            padding: 30px;
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        h1 {
            color: #333;
            text-align: center;
            margin-bottom: 10px;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 14px;
            margin-bottom: 20px;
        }

        .status {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            padding: 12px;
            border-radius: 10px;
            font-weight: 500;
            margin-bottom: 10px;
        }

        .status.connected {
            background: #d4edda;
            color: #155724;
        }

        .status.disconnected {
            background: #f8d7da;
            color: #721c24;
        }

        .status.thinking {
            background: #fff3cd;
            color: #856404;
        }

        .status-indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            animation: pulse 2s infinite;
        }

        .status.connected .status-indicator {
            background: #28a745;
        }

        .status.disconnected .status-indicator {
            background: #dc3545;
        }

        .status.thinking .status-indicator {
            background: #ffc107;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        .latency-info {
            text-align: center;
            color: #666;
            font-size: 12px;
            margin-bottom: 10px;
        }

        .transcriptions {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            min-height: 400px;
            max-height: 500px;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 15px;
        }

        .message {
            padding: 12px 16px;
            border-radius: 12px;
            max-width: 80%;
            word-wrap: break-word;
            animation: slideIn 0.3s ease-out;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .message.user {
            background: #667eea;
            color: white;
            align-self: flex-end;
            border-bottom-right-radius: 4px;
        }

        .message.assistant {
            background: #e9ecef;
            color: #333;
            align-self: flex-start;
            border-bottom-left-radius: 4px;
        }

        .message-header {
            font-size: 11px;
            font-weight: 600;
            margin-bottom: 5px;
            opacity: 0.8;
        }

        .message-content {
            font-size: 15px;
            line-height: 1.5;
        }

        .controls {
            display: flex;
            gap: 10px;
            justify-content: center;
        }

        button {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }

        button:active {
            transform: translateY(0);
        }

        .btn-start {
            background: #28a745;
            color: white;
        }

        .btn-stop {
            background: #dc3545;
            color: white;
        }

        .btn-start:hover {
            background: #218838;
        }

        .btn-stop:hover {
            background: #c82333;
        }

        .btn-start:disabled,
        .btn-stop:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .empty-state {
            text-align: center;
            color: #999;
            padding: 40px;
            font-style: italic;
        }

        /* Scrollbar styling */
        .transcriptions::-webkit-scrollbar {
            width: 8px;
        }

        .transcriptions::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 10px;
        }

        .transcriptions::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 10px;
        }

        .transcriptions::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è Low-Latency Voice Agent</h1>
        <p class="subtitle">Powered by Deepgram Voice Agent API & Gemini Flash 2.5</p>
        
        <div id="status" class="status disconnected">
            <span class="status-indicator"></span>
            <span id="statusText">Disconnected</span>
        </div>
        
        <div id="latencyInfo" class="latency-info" style="display: none;">
            Response latency: <span id="latencyValue">-</span>ms
        </div>

        <div class="transcriptions" id="transcriptions">
            <div class="empty-state">Conversation will appear here...</div>
        </div>

        <div class="controls">
            <button id="startBtn" class="btn-start" onclick="startConversation()">Start Conversation</button>
            <button id="stopBtn" class="btn-stop" onclick="stopConversation()" disabled>Stop</button>
        </div>
    </div>

    <script>
        let socket;
        let mediaStream;
        let audioContext;
        let processor;
        let outputProcessor; // For continuous audio playback
        let isConnected = false;
        let audioBuffer = []; // Continuous buffer for audio data
        let audioBufferReadIndex = 0;
        let conversationStarted = false;

        function updateStatus(status, text) {
            const statusEl = document.getElementById('status');
            const statusText = document.getElementById('statusText');
            statusEl.className = `status ${status}`;
            statusText.textContent = text;
        }

        function addMessage(role, content) {
            const transcriptions = document.getElementById('transcriptions');
            const emptyState = transcriptions.querySelector('.empty-state');
            if (emptyState) {
                emptyState.remove();
            }

            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            
            const header = document.createElement('div');
            header.className = 'message-header';
            header.textContent = role === 'user' ? 'üë§ You' : 'ü§ñ Assistant';
            
            const contentDiv = document.createElement('div');
            contentDiv.className = 'message-content';
            contentDiv.textContent = content;
            
            messageDiv.appendChild(header);
            messageDiv.appendChild(contentDiv);
            transcriptions.appendChild(messageDiv);
            
            // Auto-scroll to bottom
            transcriptions.scrollTop = transcriptions.scrollHeight;
        }

        function updateLatency(latency) {
            const latencyInfo = document.getElementById('latencyInfo');
            const latencyValue = document.getElementById('latencyValue');
            if (latency) {
                latencyInfo.style.display = 'block';
                latencyValue.textContent = latency;
            } else {
                latencyInfo.style.display = 'none';
            }
        }

        async function startConversation() {
            if (conversationStarted) return;

            try {
                conversationStarted = true;
                document.getElementById('startBtn').disabled = true;
                updateStatus('disconnected', 'Connecting...');

                // Try to create AudioContext at 16kHz to match Deepgram's expected input rate
                // This avoids resampling delays on input audio
                let audioContextSampleRate = 16000;
                try {
                    // Some browsers support custom sample rates
                    audioContext = new AudioContext({ sampleRate: 16000 });
                    audioContextSampleRate = audioContext.sampleRate;
                } catch (e) {
                    // Fallback to native rate if 16kHz not supported
                    audioContext = new AudioContext();
                    audioContextSampleRate = audioContext.sampleRate;
                }

                // Get microphone permission
                const constraints = {
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000,
                        echoCancellation: false,
                        noiseSuppression: false,
                        autoGainControl: false,
                        latency: 0
                    }
                };
                mediaStream = await navigator.mediaDevices.getUserMedia(constraints);

                // Connect to WebSocket server (works locally and on Render)
                const wsProtocol = window.location.protocol === 'https:' ? 'wss' : 'ws';
                const wsHost = window.location.host || 'localhost:3000';
                const wsUrl = `${wsProtocol}://${wsHost}`;
                socket = new WebSocket(wsUrl);

                socket.onopen = () => {
                    isConnected = true;
                    updateStatus('connected', 'Connected - Listening...');
                    startStreaming();
                    document.getElementById('stopBtn').disabled = false;
                };

                socket.onmessage = async (event) => {
                    // Handle text messages (transcriptions, events)
                    if (typeof event.data === 'string') {
                        try {
                            const data = JSON.parse(event.data);
                            handleTextMessage(data);
                        } catch (error) {
                            console.error('Error parsing message:', error);
                        }
                    } 
                    // Handle binary messages (audio)
                    else if (event.data instanceof Blob || event.data instanceof ArrayBuffer) {
                        try {
                            const arrayBuffer = event.data instanceof Blob 
                                ? await event.data.arrayBuffer() 
                                : event.data;
                            const audioData = new Int16Array(arrayBuffer);
                            // Convert Int16 to Float32 and add to continuous buffer
                            for (let i = 0; i < audioData.length; i++) {
                                const sample = audioData[i] / (audioData[i] >= 0 ? 0x7FFF : 0x8000);
                                audioBuffer.push(sample);
                            }
                            // Start playback if not already started
                            if (!outputProcessor) {
                                startAudioPlayback();
                            }
                        } catch (error) {
                            console.error('Error processing audio response:', error);
                        }
                    }
                };

                socket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    updateStatus('disconnected', 'Connection Error');
                };

                socket.onclose = () => {
                    isConnected = false;
                    updateStatus('disconnected', 'Disconnected');
                    // Reuse the same cleanup logic as the Stop button
                    stopConversation();
                };
            } catch (error) {
                console.error('Error initializing:', error);
                updateStatus('disconnected', 'Error: ' + error.message);
                conversationStarted = false;
                document.getElementById('startBtn').disabled = false;
            }
        }

        function handleTextMessage(data) {
            switch (data.type) {
                case 'transcription':
                    addMessage(data.role, data.content);
                    break;
                case 'user_speaking':
                    updateStatus('connected', 'You are speaking...');
                    break;
                case 'agent_thinking':
                    updateStatus('thinking', 'Agent is thinking...');
                    break;
                case 'agent_speaking':
                    updateStatus('connected', 'Agent is speaking...');
                    if (data.latency) {
                        updateLatency(data.latency);
                    }
                    break;
            }
        }

        function startStreaming() {
            if (!mediaStream || !isConnected) return;

            try {
                const source = audioContext.createMediaStreamSource(mediaStream);
                const bufferSize = 2048;
                processor = audioContext.createScriptProcessor(bufferSize, 1, 1);

                source.connect(processor);
                processor.connect(audioContext.destination);

                // Check if resampling is needed
                const actualSampleRate = audioContext.sampleRate;
                const targetSampleRate = 16000;
                const needsResampling = Math.abs(actualSampleRate - targetSampleRate) > 100;
                
                let resampleAccumulator = 0;
                let resampleBuffer = [];
                const stepSize = needsResampling ? actualSampleRate / targetSampleRate : 1;

                // Send audio frequently for low latency - small chunks, no big buffers
                let lastSendTime = 0;
                const sendInterval = 10; // Send every 10ms for very low latency

                processor.onaudioprocess = (e) => {
                    const now = Date.now();
                    const inputData = e.inputBuffer.getChannelData(0);
                    
                    if (!needsResampling) {
                        // No resampling - send directly with minimal delay
                        if (socket?.readyState === WebSocket.OPEN && now - lastSendTime >= sendInterval) {
                            const pcmData = convertFloatToPcm(inputData);
                            socket.send(pcmData.buffer);
                            lastSendTime = now;
                        }
                    } else {
                        // Resample with minimal buffering - send very small chunks
                        for (let i = 0; i < inputData.length; i++) {
                            resampleAccumulator += stepSize;
                            if (resampleAccumulator >= 1.0) {
                                resampleBuffer.push(inputData[i]);
                                resampleAccumulator -= 1.0;
                            }
                        }
                        
                        // Send tiny chunks frequently (80 samples = 5ms at 16kHz) for instant response
                        if (socket?.readyState === WebSocket.OPEN && 
                            (resampleBuffer.length >= 80 || (now - lastSendTime >= sendInterval && resampleBuffer.length > 0))) {
                            const samplesToSend = Math.min(80, resampleBuffer.length);
                            const pcmData = convertFloatToPcm(new Float32Array(resampleBuffer.slice(0, samplesToSend)));
                            socket.send(pcmData.buffer);
                            resampleBuffer.splice(0, samplesToSend);
                            lastSendTime = now;
                        }
                    }
                };
            } catch (error) {
                console.error('Error starting audio stream:', error);
            }
        }

        function convertFloatToPcm(floatData) {
            const pcmData = new Int16Array(floatData.length);
            for (let i = 0; i < floatData.length; i++) {
                const s = Math.max(-1, Math.min(1, floatData[i]));
                pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return pcmData;
        }

        function startAudioPlayback() {
            if (outputProcessor || !audioContext) return;

            try {
                // Ensure audio context is running
                if (audioContext.state === 'suspended') {
                    audioContext.resume();
                }

                // Create a ScriptProcessorNode for continuous playback
                // Buffer size: 4096 samples for low latency
                const bufferSize = 4096;
                outputProcessor = audioContext.createScriptProcessor(bufferSize, 0, 1);
                
                const nativeSampleRate = audioContext.sampleRate;
                const inputSampleRate = 16000;
                // Ratio: how many input samples per output sample
                // If native is 44100 and input is 16000, we need 16000/44100 = 0.3628 input samples per output sample
                const inputToOutputRatio = inputSampleRate / nativeSampleRate;

                // Resampling state - tracks position in input buffer (in input samples)
                let inputPosition = 0;

                outputProcessor.onaudioprocess = (e) => {
                    const output = e.outputBuffer.getChannelData(0);
                    const outputLength = output.length;

                    // Resample from 16kHz to native sample rate
                    for (let i = 0; i < outputLength; i++) {
                        // Calculate source index in input buffer
                        const sourceIndex = inputPosition;
                        const sourceIndexFloor = Math.floor(sourceIndex);
                        const sourceIndexFrac = sourceIndex - sourceIndexFloor;

                        if (sourceIndexFloor + 1 < audioBuffer.length) {
                            // Linear interpolation for smooth resampling
                            const sample1 = audioBuffer[sourceIndexFloor];
                            const sample2 = audioBuffer[sourceIndexFloor + 1];
                            output[i] = sample1 + (sample2 - sample1) * sourceIndexFrac;
                        } else if (sourceIndexFloor < audioBuffer.length) {
                            output[i] = audioBuffer[sourceIndexFloor];
                        } else {
                            output[i] = 0; // Silence if no data
                        }

                        // Move forward in input buffer by the ratio
                        // For each output sample, we consume inputToOutputRatio input samples
                        inputPosition += inputToOutputRatio;
                    }

                    // Remove consumed samples from buffer
                    const consumedSamples = Math.floor(inputPosition);
                    if (consumedSamples > 0) {
                        audioBuffer.splice(0, consumedSamples);
                        inputPosition = inputPosition - consumedSamples; // Keep fractional part
                    }

                    // If buffer is getting too large, trim it (shouldn't happen in normal operation)
                    if (audioBuffer.length > 48000) { // ~3 seconds at 16kHz
                        audioBuffer.splice(0, audioBuffer.length - 16000);
                    }
                };

                outputProcessor.connect(audioContext.destination);
            } catch (error) {
                console.error('Error starting audio playback:', error);
                outputProcessor = null;
            }
        }

        function stopConversation() {
            conversationStarted = false;
            audioBuffer = [];
            audioBufferReadIndex = 0;
            
            if (outputProcessor) {
                outputProcessor.disconnect();
                outputProcessor = null;
            }
            
            if (processor) {
                processor.disconnect();
                processor = null;
            }
            
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            if (socket) {
                socket.close();
            }
            
            updateStatus('disconnected', 'Disconnected');
            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
            updateLatency(null);
        }

        // Clean up when the page is closed
        window.onbeforeunload = () => {
            stopConversation();
        };
    </script>
</body>
</html>

